\documentclass{beamer}

\setbeamertemplate{section in toc}[sections numbered]
\setbeamertemplate{subsection in toc}[subsections numbered]

% american mathematical society
\usepackage{amsmath,amsthm,amssymb}

% reversed cases enviroment
\newenvironment{rcases}{\left.\begin{aligned}}{\end{aligned}\right\rbrace}

% drawing functionality
\usepackage{tikz}
\usetikzlibrary{matrix,patterns}

% equation numbering
\numberwithin{equation}{section}

% theorem types
\newtheorem{proposition}{Proposition}

% math operators
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\var}{Var}
\DeclareMathOperator*{\cov}{Cov}
\DeclareMathOperator{\VaR}{VaR}
\DeclareMathOperator{\cvar}{CVaR}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\dist}{dist}

% additional mathematical fonts
\usepackage{mathrsfs}
\usepackage{bbm}

% remove navigation bar
\beamertemplatenavigationsymbolsempty

% add slide numbers
\setbeamertemplate{footline}[frame number]

% title
\title{Measure Theory}
\subtitle{Exam}
\author{Kasper Rosenkrands}
\institute{Aalborg University}
\date{E20}

% toc at each section
\AtBeginSection[]
{
  \begin{frame}
    \frametitle{Table of Contents}
    \tableofcontents[currentsection, hideothersubsections]
  \end{frame}
}

% definition of comment function
\newcommand{\comment}[1]{
    \begin{center}
        \colorbox{yellow}{
            \textsf{
                \textbf{#1}
            }
        }
    \end{center}
}
\newcommand{\task}[1]{
    \begin{center}
        \colorbox{red}{
            \textsf{
                \textbf{#1}
            }
        }
    \end{center}
}

\newenvironment{frame2}{\begin{frame}\frametitle{{\normalsize \secname} \\ {\large \subsecname}}}{\end{frame}}

\begin{document}

\frame{\titlepage}

\begin{frame}
\frametitle{Table of Contents}
\tableofcontents[hideallsubsections]
\end{frame}

\section{Conditional Expectation}

\subsection{Strategy}
\begin{frame2}
    \begin{itemize}
        \item Definition of conditional expectation
        \item Remark 4.46
        \item Exercise 4.47
        \item Exercise 4.48
        \item Exercise 4.51
    \end{itemize}
\end{frame2}

\subsection{Defintion of Conditional Expectation}

\begin{frame2}
    \textbf{Definition 4.42.}
    Let $\mathcal{A} \subset \mathcal{F}$ be a sub-$\sigma$-algebra of $\mathcal{F}$ and let $f \, : \, \Omega \rightarrow \mathbb{R}$ be $\mathbb{P}$-integrable.
    Then a function $g \, : \, \Omega \rightarrow \mathbb{R}$ is called a \textit{representative of the conditional expectation of f under the hypothesis} $\mathcal{A}$, iff it is $\mathbb{P}$-integrable, $\mathcal{A}$-\textit{measurable}, and satisfies
    \begin{align}\label{eq:4.15}
        \int_Af\, d\mathbb{P} = \int_Ag\, d\mathbb{P}, \quad A \in \mathcal{A}.
    \end{align}
\end{frame2}

\subsection{Remark}

\begin{frame2}
    \textbf{Remark 4.46.}
    Substituting $A = \Omega$ in \eqref{eq:4.15}, we obtain
    \begin{align}
        \int_\Omega f\, d\mathbb{P} &= \int_\Omega g\, d\mathbb{P} \\
        \mathbb{E}[f] &= \mathbb{E}\left[g\right] \\
        \mathbb{E}[f] &= \mathbb{E}\left[\mathbb{E}^\mathcal{A}\left[f\right]\right],
    \end{align}
    for all sub-$\sigma$-algebras $\mathcal{A} \subset \mathcal{F}$ and all $\mathbb{P}$-integrable $f \, : \, \Omega \rightarrow \mathbb{C}$.
\end{frame2}

\subsection{Exercise 4.47}

\begin{frame2}
    \textbf{Exercise.}
    Recall that $\{\emptyset,\Omega\}$ is a sub-$\sigma$-algebra of $\mathcal{F}$.
    Convince yourself of the validity of the formula
    \begin{align}
        \mathbb{E}^{\{\emptyset,\Omega\}}[f] = \mathbb{E}[f],\quad \mathbb{P}-\text{a.s.,}
    \end{align}
    for every $\mathbb{P}$-integrable $f \, : \, \Omega \rightarrow \mathbb{R}$.

    \vspace{10pt}
    \textbf{Solution.}
    By defintion
    \begin{align}
        \int_Af\, d\mathbb{P} = \int_Ag\, d\mathbb{P}, \quad A \in \{\emptyset,\Omega\}.
    \end{align}
\end{frame2}

\begin{frame2}
    \begin{align}
        \int_\Omega f\, d\mathbb{P} = \mathbb{E}\left[f\right] = \mathbb{E}\left[\mathbb{E}^{\{\emptyset,\Omega\}}\left[f\right]\right] = \int_\Omega g\, d\mathbb{P}.
    \end{align}
    \begin{align}
        \int_\emptyset f\, d\mathbb{P} = 0 = \int_\emptyset g\, d\mathbb{P}.
    \end{align}
\end{frame2}

\subsection{Exercise 4.48}

\begin{frame2}
    \textbf{Exercise.}
    \textit{Linearity.}
    Let $\mathcal{A}$ be a sub-$\sigma$-algebra of $\mathcal{F}$, $\alpha,\beta \in \mathbb{R}$, and let $f,g \, : \, \Omega \rightarrow \mathbb{R}$ be $\mathbb{P}$-integrable.
    Show that
    \begin{align}
        \mathbb{E}^\mathcal{A}[\alpha f + \beta g] = \alpha \mathbb{E}^\mathcal{A}[ f] + \beta\mathbb{E}^\mathcal{A}[g], \quad \mathbb{P}-\text{a.s.} 
    \end{align}

    \vspace{10pt}
    \textbf{Solution.}
    \begin{align*}
        &\quad\int_A \mathbb{E}^{\mathcal{A}}\left[\alpha f + \beta g\right] \, d\mathbb{P} = \int_A \alpha f + \beta g \, d\mathbb{P}\\
        % &\quad \mathbb{E}^{\mathcal{A}}\left[\alpha f + \beta g\right]  = \alpha f + \beta g  \\
        &= \int_A \alpha f \, d\mathbb{P} + \int_A \beta g \, d\mathbb{P} =  \alpha \int_A f \, d\mathbb{P} +  \beta \int_A g \, d\mathbb{P} \\
        &= \alpha \int_A \mathbb{E}^{\mathcal{A}}\left[f\right] \, d\mathbb{P} +  \beta \int_A \mathbb{E}^{\mathcal{A}}\left[g\right] \, d\mathbb{P} =  \int_A \alpha \mathbb{E}^{\mathcal{A}}\left[f\right] +  \beta \mathbb{E}^{\mathcal{A}}\left[g\right] \, d\mathbb{P}.
    \end{align*}
\end{frame2}

\subsection{Exercise 4.51}

\begin{frame2}
    \textbf{Exercise.}
    Let $m,n \in \mathbb{N}$, $m < n$, and $X_1, \ldots, X_n \, : \, \Omega \rightarrow \mathbb{R}$ be independent $\mathbb{P}$-integrable random variables.
    Let
    \begin{align}
        \mathcal{F}_m&:=\sigma(X_1,\ldots,X_m)\\
        &:=\sigma\left(\left\{X_i^{-1}(B)\,|\,B\in\mathcal{B}(\mathbb{R}),\, i \in \{1,\ldots,m\} \right\}\right)
    \end{align}
    denote the $\sigma$-algebra induced by $X_1,\ldots,X_m$.
    Show that
    \begin{align}
        \mathbb{E}^{\mathcal{F}_m}\left[ \sum_{i = 1}^n X_i \right] &= \sum_{i=1}^m X_i + \sum_{j = m+1}\mathbb{E}\left[X_j\right], \\
        \mathbb{E}^{\mathcal{F}_m}\left[ \prod_{i = 1}^n X_i \right] &= \left(\prod_{i=1}^m X_i\right)\left(\prod_{j=m+1}^n \mathbb{E}[X_j]\right).
    \end{align}

    \vspace{10pt}
\end{frame2}

\begin{frame2}
    \textbf{Solution.}
    For the first part we have that
    \begin{align}
        \text{for } i \leq m&: \ X_i \in \mathcal{M}(\mathcal{F}^m) \Rightarrow \mathbb{E}\left[X_i|\mathcal{F}^m\right] = X_i \\
        \text{for } j > m&: \ X_j \perp \mathcal{F}^m \Rightarrow \mathbb{E}\left[X_j|\mathcal{F}^m\right] = \mathbb{E}[X_j].
    \end{align}
    Thus we have that
    \begin{align}
        \mathbb{E}^{\mathcal{F}^m}\left[ \sum_{i = 1}^n X_i \right] &= \sum_{i=1}^m X_i + \sum_{j = m+1}\mathbb{E}\left[X_j\right].
    \end{align}
\end{frame2}

\begin{frame2}
    For the second part we have that, because $X_1,\ldots,X_m \in \mathcal{M}(\mathcal{F}^m)$ and by independence
    \begin{align}
        \mathbb{E}^{\mathcal{F}^m}\left[ \prod_{i = 1}^n X_i \right] &= \left(\prod_{i=1}^m X_i\right) \mathbb{E}^{\mathcal{F}^m}\left[\prod_{j=m+1}^n X_j\right] \\
        &= \left(\prod_{i=1}^m X_i\right) \mathbb{E}\left[\prod_{j=m+1}^n X_j\right] \\
        &= \left(\prod_{i=1}^m X_i\right)\left(\prod_{j=m+1}^n \mathbb{E}[X_j]\right).
    \end{align}
\end{frame2}

\section{Brownian Motion}

\subsection{Strategy}
\begin{frame2}
    \begin{itemize}
        \item Defintion of Brownian motion
        \item Exercise 5.43
        \item Exercise 5.44
    \end{itemize}
\end{frame2}

\subsection{Definition}

\begin{frame2}
    % \task{Define $\nu$} 
    \textbf{Definition 5.38.} Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space and $d\in\mathbb{N}$.
    Then a stochastic process $B = (B_t)_{t\geq 0}$ on $(\Omega,\mathcal{F},\mathbb{P})$ with target space $(\mathbb{R}^d,\mathcal{B}(\mathbb{R}^d))$ and parameter set $[0,\infty)$ is called a $d$-dimensional \textit{standard Brownian motion} or, synonymously, \textit{Wiener process} on $(\Omega,\mathcal{F},\mathbb{P})$, iff it has the following four properties:
    \begin{enumerate}
        \item All its paths $B_\bullet(\omega)$, $\omega \in \Omega$, are continuous.
        \item $\mathbb{P}(\{B_0 = 0\})=1$ % or, equivalently, $\mathbb{P} \circ B^{-1}_0 = \nu_0$.
        \item $B$ has independent and stationary increments.
        \item For all $t > s \geq 0$, the distribution of $B_t - B_s$ is a $d$-dimensional centered Gaussian distribution with covariance matrix $(t-s)\mathbbm{1}_d$. % or, in symbols
        % \begin{align}
        %     \mathbb{P} \circ (B_t - B_s)^{-1} = \nu_{t - s}.
        % \end{align} 
    \end{enumerate}
\end{frame2}

\subsection{Exercise 5.43}

\begin{frame2}
    \textbf{Exercise.} Compute the probabilities $\mathbb{P}(\{B_1 = 0\})$ and $\mathbb{P}(\{B_\bullet = f\})$ for $f \in C([0,\infty),\mathbb{R}^d)$.

    \vspace{10pt}
    \textbf{Solution.} 
    For the first part we have
    \begin{align*}
        \mathbb{P}(\{B_1 = 0\}) &= \mathbb{P}(\{B_1 - B_0 = 0\}) \\
        &= \int_0^0 \frac{e^{-\|x\|^2/2(1-0)}}{(2\pi(1 - 0))^{d/2}}\,\text{d}x = \int_0^0 \frac{e^{-\|x\|^2/2}}{(2\pi)^{d/2}}\,\text{d}x = 0.
    \end{align*}
    For the second part let us first look at the event itself
    \begin{align}
        \{B_\bullet = f\} = \{\omega \in \Omega \ | \ \forall t\geq 0\,:\, B_t = f(t)\}.
    \end{align}
\end{frame2}

\begin{frame2}
    Further we observe that
    \begin{align}
        \{B_1 = f(1)\} \subset \{B_\bullet = f\}.
    \end{align}
    Notice then that
    \begin{align}
        P(B_1 = f(1)) = 0 \quad \Rightarrow \quad P(B_\bullet = f) = 0.
    \end{align}
\end{frame2}

\subsection{Exercise 5.44}

\begin{frame2}
    \textbf{Exercise.} 
    Let $B$ be a one-dimensional standard Brownian motion on the probability space $(\Omega, \mathcal{F}, \mathbb{P})$ and let $\mathbb{E}[\cdot]$ denote the expectation with respect to $\mathbb{P}$.
    Show that $\mathbb{E}[B_t] = 0$ and $\mathbb{E}[B_s B_t] = s \wedge t$ for all $s,t \geq 0$.

    \vspace{10pt}
    \textbf{Solution.}
    To show the first part observe that from 2.\! we have that
    \begin{align}
        B_t = B_t - B_0.
    \end{align}
    It then follows from 4.\! that
    \begin{align}
        B_t - B_0 \sim N(0,t).
    \end{align}
\end{frame2}

\begin{frame2}
    For the second part observe that from 3.\! $B$ has independent increments.
    Without loss of generality assume that $s \leq t$.
    Then we can rewrite in the following way
    \begin{align}
        \mathbb{E}[B_sB_t] &= \mathbb{E}[B_s((B_t - B_s) + B_s)] \\
        &= \mathbb{E}[B_s(B_t - B_s)] + \mathbb{E}[B_s^2].
    \end{align}
    Because $B_s = B_s - B_0$ from 2., we have that
    \begin{align}
        \mathbb{E}[B_s(B_t - B_s)] &= \mathbb{E}[(B_s - B_0)(B_t - B_s)] \\
        &= \mathbb{E}[(B_s - B_0)]\mathbb{E}[(B_t - B_s)] = 0.
    \end{align} 
    Where the second equality follows from the two increments being independent.
    Finally we have
    \begin{align}
        \mathbb{E}[B_sB_t] = \mathbb{E}[B_s^2] = s.
    \end{align}
\end{frame2}

\section{Martingales \& Quadratic Variation}

\subsection{Strategy}
\begin{frame2}
    \begin{itemize}
        \item Introduce the martingale definition
        % \item Any additional concepts needed
        \item Show that one-dimensional Brownian motion is a martingale
        \item Defintion of Quadratic Variation
        \item Find the Quadratic Variation of Brownian motion
    \end{itemize}
 \end{frame2}

\subsection{Martingale definition}

% \begin{frame2}
%     \task{Define these things}
%     \begin{itemize}
%         \item Brownian Motions (only briefly as this is topic 2)
%     \end{itemize}
% \end{frame2}

\begin{frame2}
    Let $(M_t)_{t\in I}$ be an adapted $\mathbb{R}$-valued stochastic process such that $M_t: \ \Omega \rightarrow \mathbb{R}$ is $\mathbb{P}$-integrable for every $t\in I$.
    \begin{itemize}
        \item<-1> $M$ is called a $(\mathcal{F}_t)_{t\in I}$-martingale iff
        \begin{align}
            \mathbb{E}^{\mathcal{F}_s}[M_t] = M_s, \quad \mathbb{P}-\text{a.s.\! for all } s,t \in I \text{ with } s\leq t.
        \end{align}
        \item<2-> $M$ is called a $(\mathcal{F}_t)_{t\in I}$-submartingale iff
        \begin{align}
            \mathbb{E}^{\mathcal{F}_s}[M_t] \geq M_s, \quad \mathbb{P}-\text{a.s.\! for all } s,t \in I \text{ with } s\leq t.
        \end{align}
        \item<3-> Analogously for supermartingale.
    \end{itemize}
\end{frame2}

\subsection{One-dimensional Brownian Motion}

\begin{frame2}
    If $B$ is a one-dimensional $(\mathcal{F}_t)_{t\geq 0}$-Brownian motion, then $B$ is a $(\mathcal{F}_t)_{t\geq 0}$-martingale.

    In particular, every one-dimensional standard Brownian motion $B$ is a martingale with respect to its natural filtration $(\mathcal{B}_t^B)_{t\geq 0}$.
    % \vspace{5pt}
    % \hrule
    % \vspace{5pt}
    % %\task{Repeat Remark 6.7}
    % Remark 6.7. Let $(M_t)_{t \in I}$ be a $(\mathcal{F}_t)_{t\in I}$-martingale and denote by $(\Omega, \tilde{\mathcal{F}},(\tilde{\mathcal{F}}_t)_{t\in I},\tilde{\mathbb{P}})$ the completion of $(\Omega, \mathcal{F},(\mathcal{F}_t)_{t\in I},\mathbb{P})$.
    % Then $(M_t)_{t\in I}$ is a $(\tilde{\mathcal{F}}_t)_{t\in I}$-martingale as well, provided that $(\Omega, \tilde{\mathcal{F}},(\tilde{\mathcal{F}}_t)_{t\in I},\tilde{\mathbb{P}})$ is chosen as the underlying filtered probability space.
    % \vspace{5pt}
    % \hrule
    % \vspace{5pt}
    % By virtue of Remark 6.7 we can further conclude that every one-dimensional standard Brownian motion $B$ is a martingale on the standard filtered probability space $(\Omega, \tilde{\mathcal{F}},(\tilde{\mathcal{F}_t^B})_{t\geq 0},\tilde{\mathbb{P}})$ obtained by completing $(\Omega, \mathcal{F},(\mathcal{F}_t^B)_{t\geq 0},\mathbb{P})$.
\end{frame2}

\begin{frame2}
    \begin{itemize} 
        \item Let us first recall that
        \begin{align}
            \int_{\Omega} |B_t| \, d\mathbb{P} &= \frac{1}{(2\pi t)^{1/2}}\int_\mathbb{R} |x| e^{-x^2/2t} \, dx < \infty, \\
            \mathbb{E}[B_t] &= \frac{1}{(2\pi t)^{1/2}}\int_\mathbb{R}xe^{-x^2/2t} \, dx = 0,
        \end{align}
        for every $t > 0$.
    \end{itemize}
\end{frame2}

\begin{frame2}
    \begin{itemize}
        \item Since $B_0 = 0$, $\mathbb{P}-$a.s., it follows in particular that $B_t$ is $\mathbb{P}$-integrable with $\mathbb{E}[B_t] = 0$ for all $t \geq 0$.
        \item For all $0 \leq s \leq t < \infty$, we further observe that, $\mathbb{P}-$a.s.,
        \begin{align}
            \mathbb{E}^{\mathcal{F}_s}[B_t] &= \mathbb{E}^{\mathcal{F}_s}[B_t - B_s] + \mathbb{E}^{\mathcal{F}_s}\left[B_s\right] \\
             &= \mathbb{E}[B_t - B_s] + B_s \\ 
             &= \mathbb{E}[B_t] - \mathbb{E}[B_s] + B_s \\
             &= B_s, 
        \end{align}
        since the increment $B_t - B_s$ is $\mathcal{F}_s$-independent and $B_s$ is $\mathcal{F}_s$-measurable.
    \end{itemize}
\end{frame2}

\subsection{Definition of Quadratic Variation}

\begin{frame2}
    \textbf{Theorem (and definition) 6.50.}
    Assume that $(\Omega, \mathcal{F}, (\mathcal{F}_t)_{t\geq 0},\mathbb{P})$ satisfies the usual hypotheses and let $M$ be a continuous local martingale.
    Then there exists an up to indishtinguishability unique finite variation process $\langle M \rangle = \left(\langle M \rangle_t\right)_{t\geq 0}$ such that $\langle M \rangle_0 = 0$, $\mathbb{P}$-a.s., and
    \begin{align}
        M^2 - \langle M \rangle = \left(M_t^2 - \langle M \rangle_t \right)_{t \geq 0} \text{ is a local martingale.}
    \end{align}
    This process is called quadratic variation of $M$.
    The quadratic variation $\langle M \rangle$ can be chosen such that it is a non-drecreasing process.
    Furthermore,
    \begin{align}
        \langle M^\tau \rangle = \langle M \rangle^\tau,
    \end{align}
    up to indishtinguishability for every stopping time $\tau \, : \, \Omega \rightarrow [0,\infty]$.
    Finally, again up to indishtinguishability,
    \begin{align}
        \langle M \rangle = \langle M - M_0 \rangle.
    \end{align} 
\end{frame2}

\subsection{Quadratic Variation of 1-dimensional standard Brownian motion}

\begin{frame2}
    \textbf{Exercise 6.51.}
    Let $B=(B_t)_{t\geq 0}$ be a 1-dimensional standard Brownian motion.
    Show that
    \begin{align}
        \langle B \rangle_t = t, \quad t\geq 0.
    \end{align}
    
    \vspace{10pt}
    \textbf{Solution.}
    Define the continuous finite variation process $(t)_{t\geq 0}$. 
    Note that, obviously, $\langle B \rangle_0 = 0$.
    Thus what is left to show is that $(B_t^2-t)_{t\geq 0}$ is a local martingale.
\end{frame2}

\begin{frame2}
    First of all note that $B_t^2-t$ is $\mathbb{P}$-integrable and adapted.
    For $t > s$ we have that
    \begin{align*}
        \mathbb{E}^{\mathcal{F}_s}\left[B_t^2-t\right]&= \mathbb{E}^{\mathcal{F}_s}\left[(B_s+[B_t-B_s])^2\right]-t \\
        &= \mathbb{E}^{\mathcal{F}_s}\left[B_s^2\right] +2\mathbb{E}^{\mathcal{F}_s}[B_s(B_t-B_s)] + \mathbb{E}\left[(B_t-B_s)^2\right]-t \\
        &=B_s^2 + 2B_s \mathbb{E}^{\mathcal{F}_s}[B_t-B_s]+\mathbb{E}\left[(B_t-B_s)^2\right]-t \\
        &= B_s^2+2B_s\mathbb{E}[B_t-B_s]+\mathbb{E}\left[B_{t-s}^2\right]-t \\
        &=B_s^2 + t - s - t \\
        &= B_s^2 - s,
    \end{align*}
    showing that $B_t^2-t$ is in fact a martingale.
\end{frame2}

\section{Stochastic Integrals}

\subsection{Strategy}
\begin{frame2}
    \begin{itemize}
        \item Definition 6.34 (Elementary stochastic integral)
        \item Theorem 7.4 (Approximation by bounded simple processes)
        \item 7.2.2 Extension of the stochastic integral by isometry
    \end{itemize}
\end{frame2}

\subsection{Definition 6.34 (Elementary stochastic integral)}

\begin{frame2}
    \begingroup
    \footnotesize
    \textbf{Defintion 6.34.}
    Let $Y = (Y_t)_{t\geq 0}$ be some progressively measurable process with target space $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$ and let $X$ be a predictable step process having the representation
    \begin{align}
        X_t = \sum_{k = 1}^\infty \xi_k \chi_{\{\tau_{k - 1} < t \leq \tau_k\}}, \ t \geq 0.
    \end{align}
    for a non decreasing series of stopping times and some $\mathcal{F}_{\tau_{k-1}}$-measurable $\xi_k \, : \, \Omega \to \mathbb{R}$.
    Then the stochastic integral with integrand $X$ with respect to the intregrator $Y$ is the stochastic process
    \begin{align}
        \int X \, dY := \left(\int_0^t X_s \, d Y_s\right)_{t \geq 0}
    \end{align}
    defined by
    \begin{align}
        \int_0^t X_s \, dY_s := \sum_{k = 1}^\infty \xi_k \left(Y_{\tau_k \wedge t} - Y_{\tau_{k- 1} \wedge t}\right).
    \end{align}
    \endgroup
\end{frame2}


\subsection{Theorem 7.4}

\begin{frame2}
    \textbf{Theorem 7.4.}
    Let $(\Omega, \mathcal{F}, (\mathcal{F}_t)_{t \geq 0},\mathbb{P})$ be a filtered probability space satisfyisng the ususal hypotheses and $M$ a continuous square-integrable martingale.
    Let $X$ be a progressively measurable stochastic process satisfying
    \begin{align}
        \mathbb{E}\left[\int_0^t X_s^2 \, d \langle M \rangle_s \right] < \infty, \ t \geq 0.
    \end{align}
    Then there exists a sequence $(X^{[n]})_{n\in \mathbb{N}}$ of bounded simple processes such that
    \begin{align}
        \lim_{n \to \infty} \mathbb{E}\left[\int_0^t (X_s^{[n]} - X_s)^2 \, d\langle M \rangle_s\right] = 0.
    \end{align}
\end{frame2}

\subsection{Extension of the stochastic integral}

\begin{frame2}
    Let $M$ be a continuous square integrable martingale and $X$ a progressively measurable process satisfying
    \begin{align*}
        \mathbb{E}\left[\int_0^t X_s^2 \, d \langle M \rangle_s \right] < \infty, \ t \geq 0.
    \end{align*}
    Theorem 7.4 then gives a sequence $(X^{[n]})_{n\in \mathbb{N}}$ of bounded predictable step processes satisfying
    \begin{align*}
        \lim_{n \to \infty} \mathbb{E}\left[\int_0^t (X_s^{[n]} - X_s)^2 \, d\langle M \rangle_s\right] = 0.
    \end{align*}
\end{frame2}

\begin{frame2}
    By the Minkowski inequalities we get that
    \begin{align*}
        &\mathbb{E}\left[\int_0^t (X_s^{[m]} - X^{[n]}_s)^2 \, d\langle M \rangle_s\right]^{1/2} \\
        &\leq \mathbb{E}\left[\int_0^t (X_s^{[m]} - X_s)^2 \, d\langle M \rangle_s\right]^{1/2}  + \mathbb{E}\left[\int_0^t (X_s - X_s^{[n]})^2 \, d\langle M \rangle_s\right]^{1/2}.
    \end{align*}
    Taking limits entails that
    \begin{align}
        \mathbb{E}\left[\int_0^t (X_s^{[m]} - X^{[n]}_s)^2 \, d\langle M \rangle_s\right] \xrightarrow{m,n\to \infty} 0.
    \end{align}
\end{frame2}

\begin{frame2}
    \textbf{Lemma 7.3.}
    Let $X$ be a bounded step process and $M = (M_t)_{t \geq 0}$ a continuous square-integrable martingale.
    Then we have the isometry
    \begin{align}
        \mathbb{E}\left[\left(\int_0^t X_s \, d M_s\right)^2\right] = \mathbb{E}\left[\int_0^t X_s^2 \, d\langle M \rangle_s\right], \ t \geq 0.
    \end{align}
\end{frame2}

\begin{frame2}
    By linearity and isometry (Lemma 7.3) of the elementary stochastic integral we can rewrite
    \begin{align*}
        &\mathbb{E}\left[\int_0^t (X_s^{[m]} - X^{[n]}_s)^2 \, d\langle M \rangle_s\right] \\
        =\ &\mathbb{E}\left[\left(\int_0^t X_s^{[m]} - X^{[n]}_s \, dM_s\right)^2\right] \\
        =\ &\mathbb{E}\left[\left(\int_0^t X_s^{[m]}\, dM_s - \int_0^tX^{[n]}_s \, dM_s\right)^2\right].
    \end{align*}
    Thus 
    \begin{align}
        \mathbb{E}\left[\left(\int_0^t X_s^{[m]}\, dM_s - \int_0^tX^{[n]}_s \, dM_s\right)^2\right] \xrightarrow{m,n\to\infty}0
    \end{align}
\end{frame2}

\begin{frame2}
    \begingroup
    \footnotesize
    \textbf{Theorem 6.32.}
    Consider the case $I = [0,\infty)$ and assume that $(\Omega, \mathcal{F}, (\mathcal{F}_t)_{t \geq 0},\mathbb{P})$ satisfies the usual hypotheses.
    Let $M^{[n]} = (M_t^{[n]})_{t\geq 0}$ be a right-continuous, square-integrable martingale for every $n \in \mathbb{N}$.
    Assume that
    \begin{align}
        \mathbb{E}\left[(M_t^{[p]} - M_t^{[n]})^2\right] \xrightarrow{n, p \to\infty} 0, \ t \geq 0.
    \end{align}
    Then there exists a càdlàg square-integrable martingale $M = \left(M_t\right)_{t \geq 0}$ such that
    \begin{align}
        \lim_{n \to \infty} \mathbb{E}\left[(M_t^{[n]} - M_t)^2\right] = 0, \ t \geq 0.
    \end{align}
    Every other martingale $M'$ having these properties is indishtinguishable from $M$.
    Furthermore, there exists integers $ 1 \leq n_1 < n_2 < \ldots$ and some $\mathbb{P}$-zero set $N\in \mathcal{F}$ such that
    \begin{align}
        \sup_{s \in [0,t]} \left\|M_s^{[n_j]}(\omega) - M_s(\omega) \right\| \xrightarrow{j \to \infty} 0, \ \omega \in \Omega\backslash N, \, t \geq 0.
    \end{align}
    In particular, if all $M^{[n]}$ with $n \in \mathbb{N}$ are continuous, then $M$ can be chosen continuous as well.
    \endgroup
\end{frame2}

\begin{frame2}
    Theorem 6.32 thus gives a continuous square integrable martingale $(I_t)_{t\geq 0}$ such that
    \begin{align}
        \mathbb{E}\left[\left(\int_0^t X_s^{[m]}\, dM_s - I_t\right)^2\right] \xrightarrow{m\to\infty}0,
    \end{align}
    and every martingale $I'$ having these properties are indishtinguishable from $I$.
\end{frame2}

\begin{frame2}
    Next we can verify that $I$, does not depend on the choice of approximating sequence.

    \vspace{10pt}
    Let $(Y^{[n]})_{n \in \mathbb{N}}$ be another bounded predictable step process such that
    \begin{align}
        \lim_{n\to\infty}\mathbb{E}\left[\int_0^t \left(Y_s^{[n]} - X_s\right)^2 \, d\langle M \rangle_s\right] = 0.
    \end{align}
    Let $(J_t)_{t\geq 0}$ be a corresponding continuous square-integrable martingale satisfying
    \begin{align}
        \mathbb{E}\left[\left(\int_0^t Y_s^{[n]}\, dM_s - J_t\right)^2\right] \xrightarrow{n\to\infty}0
    \end{align}
\end{frame2}

\begin{frame2}
    For every $t\geq 0$, we then obtain
    \begin{align*}
        \mathbb{E}\left[(I_t - J_t)^2\right] &= \lim_{n\to\infty}\mathbb{E}\left[\left(\int_0^t X_s^{[n]}\, dM_s - \int_0^tY^{[n]}_s \, dM_s\right)^2\right] \\
        &=\lim_{n\to\infty}\mathbb{E}\left[\left(\int_0^t X_s^{[n]} - Y^{[n]}_s \, dM_s\right)^2\right] \\
        &=\lim_{n\to\infty}\mathbb{E}\left[\int_0^t\left( X_s^{[n]} - Y^{[n]}_s\right)^2 \, d\langle M \rangle_s\right] = 0,
    \end{align*}
    where we used isometry and
    \begingroup
    \footnotesize
    \begin{align*}
        \lim_{n \to \infty} \mathbb{E}\left[\int_0^t (X_s^{[n]} - X_s)^2 \, d\langle M \rangle_s\right] = 0, \quad
        \lim_{n\to\infty}\mathbb{E}\left[\int_0^t \left(Y_s^{[n]} - X_s\right)^2 \, d\langle M \rangle_s\right] = 0.
    \end{align*}
    \endgroup
    The processes $(I_t)_{t\geq 0}$ and $(J_t)_{t\geq 0}$ are thus modification of each other and therfore indishtinguishable as they are both continuous.
\end{frame2}

\subsection{Definition 7.8}

\begin{frame2}
    \textbf{Definition 7.8.}
    Any choice of the up to indishtinguishability well-defined and unique continuous square-integrable martingale $(I_t)_{t \geq 0}$ introduced in the preceeding argumentation is called the stochastic integral of $X$ with respect to $M$ and we write
    \begin{align*}
        \int_0^t X_s \, dM_s := I_t, \ t \geq 0, \quad \int X \, dM := \left(\int_0^t X_s \, d M_s \right)_{t\geq 0}.
    \end{align*}    
\end{frame2}

\section{Itô's Formula}

\subsection{Strategy}
\begin{frame2}
    \begin{itemize}
        \item Definition of semi martingales
        \item Itô's formula for semi martingales
        \item Exercise 8 (Part of Example 4)
    \end{itemize}
\end{frame2}

\subsection{Semi Martingales}

\begin{frame2}
    Denote by $\mathcal{M}_{c,loc}$ the linear space of $(\mathcal{F}_t)_{t \geq 0}$-continuous local martingales.
    
    \vspace{10pt}
    Denote by $\mathcal{A}_{loc}$ the space of adapted processes whose paths are continuous and locally of bounded variation.

    \vspace{10pt}
    Let $X = (X_t)_{t \geq 0}$ be a stochastic process defined on $(\Omega, \mathcal{F},(\mathcal{F}_t)_{t \geq 0}, \mathbb{P})$.
    We say that $X$ is a continuous semimartingales if $X$ can be decomposed as
    \begin{align}
        X_t = X_0 + M_t + A_t, \ t \geq 0,
    \end{align}
    where $M \in \mathcal{M}_{c,loc}$, $A \in \mathcal{A}_{loc}$.
    We will denote by $\mathcal{SM}^c$ the linear space of continuous semimartingales.
\end{frame2}

\subsection{Itô's formula for semi martingales}

\begin{frame2}
    \textbf{Theorem 7.}
    %Let $X,Y \in \mathcal{SM}^c$ with decomposition $(M,A),(\bar{M},\bar{A}) \in \mathcal{M}_{c,loc} \times A_{loc}$, respectively.
    %The mapping $H \in \mathcal{L}(X) \mapsto H \cdot X \in \mathcal{SM}^c$ enjoys the following properties (up to indishtinguishability)
    %\begin{enumerate}
        %\item 
        Let $X = (X^{(1)},\ldots,X^{(d)})$, where $X^{(i)} \in \mathcal{SM}^c$, for all $i = 1,\ldots,d$.
        If $f \, : \, \mathbb{R}^d \rightarrow \mathbb{R}$ is twice continuously differentiable and $X = (X^{(1)},\ldots,X^{(d)})$, then
        \begin{align*}
            f(X_t) = f(X_0) &+ \sum_{i = 1}^d\int_0^t\partial_{x_i}f(X_s)\, dX_s^{(i)} \\
            &\quad+ \frac{1}{2}\sum_{i,j=1}^d\int_0^t\partial_{x_i}\partial_{x_j}f(X_s)\, d \langle X^{(i)},X^{(j)}\rangle_s.
        \end{align*}
        In the case $d = 1$ we have
        \begin{align*}
            f(X_t) = f(X_0) &+ \int_0^t f'(X_s)\, dX_s + \frac{1}{2}\int_0^t f''(X_s)\, d \langle X \rangle_s.
        \end{align*}
    %\end{enumerate} 
\end{frame2}

\subsection{Exercise 8}

\begin{frame2}
    \textbf{Example 4 (Complex Exponential Process).}
    Let $M \in \mathcal{M}_{c,loc}$ starting at 0 and define for $t \geq 0$
    \begin{align}
        N_t^{(1)} := \cos(M_t)e^{\frac{1}{2}\langle M \rangle_t},\ N_t^{(2)} := \sin(M_t)e^{\frac{1}{2}\langle M \rangle_t}.
    \end{align}
    Verify that
    \begin{align}
        N_t^{(2)} = \int_0^t N_s^{(1)} \, dM_s, \ t \geq 0.
    \end{align}
\end{frame2}

\begin{frame2}
    We will start by observing that Itô's formula applied to the function $f(x,y) = xy$ results in the following integration by parts formula
    \begin{align*}
        X_tY_t = X_0Y_0 + \int_0^t X_s \, dY_s + \int_0^tY_s\, dX_s + \langle X,Y \rangle_t, \ \forall\, X,Y \in \mathcal{SM}^c.
    \end{align*}
    Thus we get that
    \begin{align}
         N_t^{(2)} &= \sin(M_t)e^{\frac{1}{2}\langle M \rangle_t} \\
        &= \int_0^t \sin(M_s) \, d\left(e^{\frac{1}{2}\langle M \rangle_s}\right) + \int_0^t e^{\frac{1}{2}\langle M \rangle_s} \, d \left(\sin(M_s)\right).
    \end{align}
\end{frame2}

\begin{frame2}
    Applying Itô's formula again we get
    \begin{align}
        e^{\frac{1}{2}\langle M \rangle_t} &= 1 + \int_0^t\partial_{\langle M \rangle}e^{\frac{1}{2}\langle M \rangle_s}\, d\langle M \rangle_s + \frac{1}{2}\int_0^t\partial^2_{\langle M \rangle} e^{\frac{1}{2}\langle M \rangle_s} \, d \langle\langle M \rangle\rangle_s \nonumber \\
        &= 1 + \frac{1}{2}\int_0^te^{\frac{1}{2}\langle M \rangle_s}\, d\langle M \rangle_s
    \end{align}
    \begin{align}
        \sin(M_t) &= \int_0^t\partial_{M}\sin(M_s)\, dM_s + \frac{1}{2}\int_0^t\partial_{M}^2\sin(M_s) \, d \langle M \rangle_s \\
        &= \int_0^t \cos(M_s) \, dM_s - \frac{1}{2}\int_0^t\sin(M_s)\, d\langle M \rangle_s.
    \end{align}
\end{frame2}

\begin{frame2}
    Using now that
    \begin{align*}
        X_t &= X_0 + \int_0^t H_s \, dY_s \\
        dX_s &= H_s \, dY_s, 
    \end{align*}
    which gives that
    \begin{align*}
        \int_0^t G_s \, d X_s = \int_0^t G_s H_s \, dY_s.
    \end{align*}
    We are able to ``rewrite''
    \begin{align*}
        d\left(e^{\frac{1}{2}\langle M \rangle_s}\right) &= \frac{1}{2}e^{\frac{1}{2}\langle M \rangle_s} \, d \langle M \rangle_s, \\
        d \left(\sin(M_s)\right) &= \cos(M_s) \, dM_s - \frac{1}{2}\sin(M_s)\, d\langle M \rangle_s.
    \end{align*}
\end{frame2}

\begin{frame2}
    Now inserting what we just found
    \begin{align*}
        N_t^{(2)} &= \sin(M_t)e^{\frac{1}{2}\langle M \rangle_t} \\
       &= \int_0^t \sin(M_s) \, d\left(e^{\frac{1}{2}\langle M \rangle_s}\right) + \int_0^t e^{\frac{1}{2}\langle M \rangle_s} \, d \left(\sin(M_s)\right) \\
       &= \frac{1}{2}\int_0^t\sin(M_s)e^{\frac{1}{2}\langle M \rangle_s}\, d\langle M \rangle_s + \int_0^t\cos(M_s)e^{\frac{1}{2}\langle M \rangle_s}\, dM_s \\
       &\qquad - \frac{1}{2}\int_0^t\sin(M_s)e^{\frac{1}{2}\langle M \rangle_s}\, d\langle M \rangle_s \\
       &= \int_0^t\cos(M_s)e^{\frac{1}{2}\langle M \rangle_s}\, dM_s \\
       &= \int_0^tN_t^{(1)} \, dM_s.
   \end{align*}
   which concludes the exercise.
\end{frame2}

\section{Girsanov-Transformation}

\subsection{Strategy}
\begin{frame2}
    \begin{itemize}
        \item Definition of the exponential martingale
        \item Novikov Theorem
        \item Girsanov Theorem
        \item Black and Scholes example
    \end{itemize}
\end{frame2}

\begin{frame2}
    \textbf{Definition 6.}
    The process
    \begin{align*}
        \mathcal{E}(X)_t:=\exp\{X_t-X_0-\langle X \rangle_t\}, \quad t\geq 0,
    \end{align*}
    is called the exponential semimartingale, when $X\in\mathcal{S}\mathcal{M}^c$, and the exponential local martingale, when $X\in\mathcal{M}_{c,loc}$.
\end{frame2}

\begin{frame2}
    \textbf{Theorem 12 (Novikov).}
    Let $M\in\mathcal{M}_{c, loc}$.
    If for every $t\geq 0$
    \begin{align*}
        \mathbb{E}(e^{\frac{1}{2}\langle M\rangle_t})<\infty,
    \end{align*}
    then $\mathcal{E}(M)_t$ is a true martingale with $\mathbb{E}(\mathcal{E}(M)_t)=1$, for all $t\geq 0$.
\end{frame2}

\begin{frame2}
    \textbf{Theorem 14 (Girsanov).}
    Let $(B_t)_{t\geq 0}$ be a Brownian motion on $(\Omega, \mathscr{F}, (\mathscr{F}_t)_{t\geq 0}, \mathbb{P})$, and $H$ a progressively measurable process such that 
    \begin{align*}
        \mathbb{E}_{\mathbb{P}}(e^{\frac{1}{2}\int_0^T H_s^2 \text{d}s})<\infty.
    \end{align*}
    Then the process defined as
    \begin{align*}
        \tilde{B}_t:=B_t-\int_0^t H_s \text{d}s, \quad 0\leq t\leq T,
    \end{align*}
    is a Brownian motion on $(\Omega, \mathscr{F}_T, (\mathscr{F}_t)_{0\leq t\leq T}, \mathbb{Q})$, where
    \begin{align*}
        d\mathbb{Q}=\mathcal{E}(H\cdot B)_T \ d \mathbb{P} = exp\left\{\int_0^T H_s \ d B_s - \frac{1}{2} \int_0^T H_s^2 \ ds\right\} \ d \mathbb{P}.
    \end{align*}
    Furthermore $\mathbb{Q} \sim \mathbb{P}$.
\end{frame2}

\begin{frame2}
    \textbf{Example 13 (Black and Scholes).}
    \begin{itemize}
        \item Consider the Black and Scholes model
        \begin{align*}
            \text{d}A_t=rA_t\text{d}t, \quad A_0=1, \quad \frac{\text{d}S_t}{S_t}=\mu \text{d}t+\sigma \text{d} B_t, \quad S_0=s_0>0.
        \end{align*}
        \item We have seen that
        \begin{align*}
            A_t&=e^{-rt},\\
            S_t&=s_0 \ e^{(\mu-\frac{\sigma^2}{2})t+\sigma B_t} \quad t \geq 0.
        \end{align*}
    \end{itemize}
\end{frame2}

\begin{frame2}
    \begin{itemize}
        \item Itô's formel kan kan simplificeres til
        \begin{align*}
            \tilde{S}_t &= e^{-rt}S_t\\
            &= x_0+\int_0^t\partial_{x_1}\tilde{S}_t\ \text{d}s + \int_0^t\partial_{x_2}\tilde{S}_t\ \text{d}B_s + \frac{1}{2}\int_0^t\partial_{x_2}^2 \tilde{S}_t \ d s
        \end{align*}
        \item Hertil finder vi de afledte
        \begin{align*}
            \tilde{S}_t =\mathrm{e}^{-rt}s_0 \ \mathrm{e}^{(\mu-\frac{\sigma^2}{2})t+\sigma B_t}
        \end{align*}
        \item $\partial_t \tilde{S}_t = \left\{ \mu - r -  \frac{\sigma^2}{2}\right\}\tilde{S}_t$
        \item $\partial_{B_t} \tilde{S}_t = \sigma \tilde{S}_t$
        \item $\partial^2_{B_t} \tilde{S}_t = \sigma^2 \tilde{S}_t$
    \end{itemize}
\end{frame2}

\begin{frame2}
    Now rewriting
    \begin{align}
        \tilde{S}_t &= e^{-rt} = x_0 + \int_0^t \sigma \tilde{S}_s\, dB_s - \int_0^t (r - \mu)\tilde{S}_s\, ds \\
        &= x_0 + \int_0^t \sigma \tilde{S}_s \, d \tilde{B}_s,
    \end{align}
    where
    \begin{align}
        \tilde{B}_t := B_t - \frac{(r - \mu)}{\sigma}t, \ t \geq 0.
    \end{align}
    Which follows from
    \begin{align}
        \tilde{S}_t &= x_0 + \int_0^t \sigma \tilde{S}_s \, d\left(B_s - \frac{r - \mu}{\sigma}s\right) \\
        &= x_0 + \int_0^t \sigma \tilde{S}_s \, dB_s - \int_0^t (r - \mu)\tilde{S}_t \, ds.
    \end{align}
\end{frame2}

\begin{frame2}
    Applying Girsanov's Theorem to
    \begin{align}
        \tilde{B}_t := B_t - \int_0^t H_s \, ds,
    \end{align}
    where $H_t := \frac{r - \mu}{\sigma}$ denotes the market price of risk.
\end{frame2}

\begin{frame2}
    Note first that, for all $t \geq 0$
    \begin{align*}
        \mathbb{E}\left[\exp\left(\frac{1}{2}\int_0^t H_s^2 \, ds\right)\right] = \mathbb{E}\left[\exp\left(\frac{1}{2}\int_0^t \frac{(r - \mu)^2}{\sigma^2} \, ds\right)\right] < \infty.
    \end{align*}
    Novikov's Condition and Girsanov's Theorem imply that the process $\tilde{B}$ is a Brownian motion w.r.t.\! to the measure
    \begin{align}
        d\mathbb{Q} = \exp\left(\int_0^T \frac{r - \mu}{\sigma}\, dB_s - \frac{1}{2}\int_0^T H_s^2 \, ds\right) \, d\mathbb{P}.
    \end{align}
    This further gives that $\mathbb{Q}$ is a local martingale measure as the discounted price process is a local martingale w.r.t. $\mathbb{Q}$
    \begin{align}
        \tilde{S}_t = s_0 + \int_0^t\sigma\tilde{S}_s \, d\tilde{B}_s,
    \end{align}
    because the stochastic integral w.r.t.\! to the Brownian motion are in general local martingales. 
\end{frame2}

\begin{frame2}
    \textbf{Definition}\newline
    A probability measure $\mathbb{Q}$ on $(\Omega,\mathscr{F}_T)$ is said to be a local martingale measure if the discounted process $(\tilde{P}_t)_{0\leq t \leq T}$ is a $\mathbb{Q}$ local martingale.

    \vspace{10pt}
    \textbf{The First Fundamental Theorem of Asset Pricing}\newline
    The market $M$ is arbitrage free, if there exists a local martingale measure $\mathbb{Q}$ that is equivalent to $\mathbb{P}$.

    \vspace{10pt}
    Thus we have shown that the Black \& Scholes model is arbitrage free.
\end{frame2}

% \begin{frame2}
%     Progressiv målelig: 
% Pointen er vi vil integrere en stokastisk proces, som er en funktion af tiden og $\omega$.
% Vi vil finde etn sandsynlighed, så
% \begin{align*}
% \mathbb{E}\left[\int_0^t X_s \ ds\right]
% \end{align*} 
% Hvilket kan skrives som
%  \begin{align*}
% \int_{\Omega} \left(\int_0^t X_s (\omega)\ ds\right) d\mathbb{P}
% \end{align*} 
% Det Lebesgue ontwegralet. 
% For at kunne betragte denne skal man skal kende dens målelighed. Ellers kan man ikke integrere. 
% \end{frame2}
% \begin{frame2}
% I forbindelse med målelighed betragter vi en $\sigma$-algebra. 
% Hele ideen kommer fra Bownian motions, hvor stierne er kontinuerte. For hver $\omega$ er de borel måelige
%  $B_t(\omega)$ som funktion af tiden t er målelige med Borel([0,t])
% Fastholder t, så vil billedet være $F_t$ måleligt. 
% Vi vil gerne have de er simultant målelige (jointly), altså vi vil have
% $F_t \otimes Borel([0,t])$, hvilket er definitionen. 
% Vi siger den den progressiv målelig hvis den er målelig mht til den sigma algebra. 
% Genrelt: Hvis en process har kontinuerte stier/cadlag er også progressiv målelig.
% \end{frame2}

\end{document}
