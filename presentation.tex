\documentclass{beamer}

\setbeamertemplate{section in toc}[sections numbered]
\setbeamertemplate{subsection in toc}[subsections numbered]

% american mathematical society
\usepackage{amsmath,amsthm,amssymb}

% reversed cases enviroment
\newenvironment{rcases}{\left.\begin{aligned}}{\end{aligned}\right\rbrace}

% drawing functionality
\usepackage{tikz}
\usetikzlibrary{matrix,patterns}

% equation numbering
\numberwithin{equation}{section}

% theorem types
\newtheorem{proposition}{Proposition}

% math operators
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\var}{Var}
\DeclareMathOperator*{\cov}{Cov}
\DeclareMathOperator{\VaR}{VaR}
\DeclareMathOperator{\cvar}{CVaR}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\dist}{dist}

% additional mathematical fonts
\usepackage{mathrsfs}
\usepackage{bbm}

% remove navigation bar
\beamertemplatenavigationsymbolsempty

% add slide numbers
\setbeamertemplate{footline}[frame number]

% title
\title{Measure Theory}
\subtitle{Exam}
\author{Kasper Rosenkrands}
\institute{Aalborg University}
\date{E20}

% toc at each section
\AtBeginSection[]
{
  \begin{frame}
    \frametitle{Table of Contents}
    \tableofcontents[currentsection, hideothersubsections]
  \end{frame}
}

% definition of comment function
\newcommand{\comment}[1]{
    \begin{center}
        \colorbox{yellow}{
            \textsf{
                \textbf{#1}
            }
        }
    \end{center}
}
\newcommand{\task}[1]{
    \begin{center}
        \colorbox{red}{
            \textsf{
                \textbf{#1}
            }
        }
    \end{center}
}

\newenvironment{frame2}{\begin{frame}\frametitle{{\normalsize \secname} \\ {\large \subsecname}}}{\end{frame}}

\begin{document}

\frame{\titlepage}

\begin{frame}
\frametitle{Table of Contents}
\tableofcontents[hideallsubsections]
\end{frame}

\section{Conditional Expectation}

\subsection{Strategy}
\begin{frame2}
    \begin{itemize}
        \item Definition of conditional expectation
        \item Remark 4.46
        \item Some of these
        \begin{itemize}
            \item Exercise 4.47
            \item Exercise 4.48
            \item Exercise 4.51
        \end{itemize}
    \end{itemize}
\end{frame2}

\subsection{Defintion of Conditional Expectation}

\begin{frame2}
    \textbf{Definition.}
    Let $\mathcal{A} \subset \mathcal{F}$ be a sub-$\sigma$-algebra of $\mathcal{F}$ and let $f \, : \, \Omega \rightarrow \mathbb{R}$ be $\mathbb{P}$-integrable.
    Then a function $g \, : \, \Omega \rightarrow \mathbb{R}$ is called a \textit{representative of the conditional expectation of f under the hypothesis} $\mathcal{A}$, iff it is $\mathbb{P}$-integrable, $\mathcal{A}$-\textit{measurable}, and satisfies
    \begin{align}\label{eq:4.15}
        \int_Af\, d\mathbb{P} = \int_Ag\, d\mathbb{P}, \quad A \in \mathcal{A}.
    \end{align}
\end{frame2}

\subsection{Remark}

\begin{frame2}
    \textbf{Remark.}
    Substituting $A = \Omega$ in \eqref{eq:4.15}, we immediately obtain
    \begin{align}
        \mathbb{E}[f] = \mathbb{E}\left[\mathbb{E}^\mathcal{A}\left[f\right]\right],
    \end{align}
    for all sub-$\sigma$-algebras $\mathcal{A} \subset \mathcal{F}$ and all $\mathbb{P}$-integrable $f \, : \, \Omega \rightarrow \mathbb{C}$.
\end{frame2}

\subsection{Exercise 4.47}

\begin{frame2}
    \textbf{Exercise.}
    Recall that $\{\emptyset,\Omega\}$ is a sub-$\sigma$-algebra of $\mathcal{F}$.
    Convince yourself of the validity of the formula
    \begin{align}
        \mathbb{E}^{\{\emptyset,\Omega\}}[f] = \mathbb{E}[f],\quad \mathbb{P}-\text{a.s.,}
    \end{align}
    for every $\mathbb{P}$-integrable $f \, : \, \Omega \rightarrow \mathbb{R}$.

    \vspace{10pt}
    \textbf{Solution.}
    \task{to do}
\end{frame2}

\subsection{Exercise 4.48}

\begin{frame2}
    \textbf{Exercise.}
    \textit{Linearity.}
    Let $\mathcal{A}$ be a sub-$\sigma$-algebra of $\mathcal{F}$, $\alpha,\beta \in \mathbb{R}$, and let $f,g \, : \, \Omega \rightarrow \mathbb{R}$ be $\mathbb{P}$-integrable.
    Show that
    \begin{align}
        \mathbb{E}^\mathcal{A}[\alpha f + \beta g] = \mathbb{E}^\mathcal{A}[\alpha f] + \mathbb{E}^\mathcal{A}[\beta g], \quad \mathbb{P}-\text{a.s.} 
    \end{align}

    \vspace{10pt}
    \textbf{Solution.}
    \task{to do}
\end{frame2}

\subsection{Exercise 4.51}

\begin{frame2}
    \textbf{Exercise.}
    Let $m,n \in \mathbb{N}$, $m < n$, and $X_1, \ldots, X_n \, : \, \Omega \rightarrow \mathbb{R}$ be independent $\mathbb{P}$-integrable random variables.
    Let
    \begin{align}
        \mathcal{F}_m&:=\sigma(X_1,\ldots,X_m)\\
        &:=\sigma\left(\left\{X_i^{-1}(B)\,|\,B\in\mathcal{B}(\mathbb{R}),\, i \in \{1,\ldots,m\} \right\}\right)
    \end{align}
    denote the $\sigma$-algebra induced by $X_1,\ldots,X_m$.
    Show that
    \begin{align}
        \mathbb{E}\left[ \sum_{i = 1}^n X_i \right] &= \sum_{i=1}^m X_i + \sum_{j = m+1}\mathbb{E}\left[X_j\right], \\
        \mathbb{E}\left[ \prod_{i = 1}^n X_i \right] &= \left(\prod_{i=1}^m X_i\right)\left(\prod_{j=m+1}^n \mathbb{E}[X_j]\right).
    \end{align}

    \vspace{10pt}
\end{frame2}

\begin{frame2}
    \textbf{Solution.}
    For the first part we have that
    \begin{align}
        \text{for } i \leq m&: \ X_i \in \mathcal{M}(\mathcal{F}^m) \Rightarrow \mathbb{E}\left[X_i|\mathcal{F}^m\right] = X_i \\
        \text{for } j > m&: \ X_j \perp \mathcal{F}^m \Rightarrow \mathbb{E}\left[X_j|\mathcal{F}^m\right] = \mathbb{E}[X_j].
    \end{align}
    Thus we have that
    \begin{align}
        \mathbb{E}\left[ \sum_{i = 1}^n X_i \right] &= \sum_{i=1}^m X_i + \sum_{j = m+1}\mathbb{E}\left[X_j\right].
    \end{align}
\end{frame2}

\begin{frame2}
    For the second part we have that, because $X_1,\ldots,X_m \in \mathcal{M}(F^m)$ and by independence
    \begin{align}
        \mathbb{E}\left[ \prod_{i = 1}^n X_i \right] &= \left(\prod_{i=1}^m X_i\right) \mathbb{E}^{\mathcal{F}^m}\left[\prod_{j=m+1}^n X_j\right] \\
        &= \left(\prod_{i=1}^m X_i\right) \mathbb{E}\left[\prod_{j=m+1}^n X_j\right] \\
        &= \left(\prod_{i=1}^m X_i\right)\left(\prod_{j=m+1}^n \mathbb{E}[X_j]\right).
    \end{align}
\end{frame2}

\section{Brownian Motions}

\subsection{Strategy}
\begin{frame2}
    \begin{itemize}
        \item Defintion of Brownian motion
        \item Exercise 5.43
        \item Exercise 5.44
    \end{itemize}
\end{frame2}

\subsection{Definition}

\begin{frame2}
    \task{Define $\nu$} 
    \textbf{Definition.} Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space and $d\in\mathbb{N}$.
    Then a stochastic process $B = (B_t)_{t\geq 0}$ on $(\Omega,\mathcal{F},\mathbb{P})$ with target space $(\mathbb{R}^d,\mathcal{B}(\mathbb{R}^d))$ and parameter set $[0,\infty)$ is called a $d$-dimensional \textit{standard Brownian motion} or, synonymously, \textit{Wiener process} on $(\Omega,\mathcal{F},\mathbb{P})$, iff it has the following four properties:
    \begin{enumerate}
        \item All its paths $B_\bullet(\omega)$, $\omega \in \Omega$, are continuous.
        \item $\mathbb{P}(\{B_0 = 0\})=1$ or, equivalently, $\mathbb{P} \circ B^{-1}_0 = \nu_0$.
        \item $B$ has independent and stationary increments.
        \item For all $t > s \geq 0$, the distribution of $B_t - B_s$ is a $d$-dimensional centered Gaussian distribution with covariance matrix $(t-s)\mathbbm{1}_d$ or, in symbols
        \begin{align}
            \mathbb{P} \circ (B_t - B_s)^{-1} = \nu_{t - s}.
        \end{align} 
    \end{enumerate}
\end{frame2}

\subsection{Exercise 5.43}

\begin{frame2}
    \textbf{Exercise.} Compute the probabilities $\mathbb{P}(\{B_1 = 0\})$ and $\mathbb{P}(\{B_\bullet = f\})$ for $f \in C([0,\infty),\mathbb{R}^d)$.

    \vspace{10pt}
    \textbf{Solution.} 
    For the first part we have
    \begin{align}
        \mathbb{P}(\{B_1 = 0\}) &= \mathbb{P}(\{B_1 - B_0 = 0\}) \\
        &= \int_0^0 \frac{e^{-\|x\|^2/2}}{(2\pi)^{d/2}}\,\text{d}x = 0.
    \end{align}
    For the second part let us first look at the event itself
    \begin{align}
        \{B_\bullet = f\} = \{\omega \in \Omega \ | \ \forall t\geq 0\,:\, B_t = f(t)\}
    \end{align}
\end{frame2}

\begin{frame2}
    Further we observe that
    \begin{align}
        \{B_1 = f(1)\} \subset \{B_\bullet = f\}.
    \end{align}
    Notice then that
    \begin{align}
        P(B_1 = f(1)) = 0 \quad \Rightarrow \quad P(B_\bullet = f) = 0.
    \end{align}
\end{frame2}

\subsection{Exercise 5.44}

\begin{frame2}
    \textbf{Exercise.} 
    Let $B$ be a one-dimensional standard Brownian motion on the probability space $(\Omega, \mathcal{F}, \mathbb{P})$ and let $\mathbb{E}[\cdot]$ denote the expectation with respect to $\mathbb{P}$.
    Show that $\mathbb{E}[B_t] = 0$ and $\mathbb{E}[B_s B_t] = s \wedge t$ for all $s,t \geq 0$.

    \vspace{10pt}
    \textbf{Solution.}
    To show the first part observe that from 2.\! we have that
    \begin{align}
        B_t = B_t - B_0.
    \end{align}
    It then follows from 4.\! that
    \begin{align}
        B_t - B_0 \sim N(0,t).
    \end{align}
\end{frame2}

\begin{frame2}
    For the second part observe that from 3. $B$ has independent increments.
    Without loss of generality assume that $s \leq t$.
    Then we can rewrite in the following way
    \begin{align}
        \mathbb{E}[B_sB_t] &= \mathbb{E}[B_s((B_t - B_s) + B_s)] \\
        &= \mathbb{E}[B_s(B_t - B_s)] + \mathbb{E}[B_s^2].
    \end{align}
    Because $B_s = B_s - B_0$ from 1., we have that
    \begin{align}
        \mathbb{E}[B_s(B_t - B_s)] &= \mathbb{E}[(B_s - B_0)(B_t - B_s)] \\
        &= \mathbb{E}[(B_s - B_0)]\mathbb{E}[(B_t - B_s)] = 0.
    \end{align} 
    Where the second equality follows fromt the two increments being independent.
    Finally we have
    \begin{align}
        \mathbb{E}[B_sB_t] = \mathbb{E}[B_s^2] = s.
    \end{align}
\end{frame2}

\section{Martingales \& Quadratic Variation}

\subsection{Strategy}
\begin{frame2}
    \begin{itemize}
        \item Introduce the martingale definition
        \item Any additional concepts needed
        \item Show that one-dimensional Brownian motion is a martingale
    \end{itemize}
 \end{frame2}

\subsection{Prerequisites}

\begin{frame2}
    \task{Define these things}
    \begin{itemize}
        \item Brownian Motions (only briefly as this is topic 2)
        \item Martingale
        \item Natural Filtration
    \end{itemize}
\end{frame2}

\begin{frame2}
    Let $(M_t)_{t\in I}$ be an adapted $\mathbb{R}$-valued stochastic process such that $M_t: \ \Omega \rightarrow \mathbb{R}$ is $\mathbb{P}$-integrable for every $t\in I$.
    \begin{itemize}
        \item<-1> $M$ is called a $(\mathcal{F}_t)_{t\in I}$-martingale iff
        \begin{align}
            \mathbb{E}^{\mathcal{F}_s}[M_t] = M_s, \quad \mathbb{P}-\text{a.s.\! for all } s,t \in I \text{ with } s\leq t.
        \end{align}
        \item<2-> $M$ is called a $(\mathcal{F}_t)_{t\in I}$-submartingale iff
        \begin{align}
            \mathbb{E}^{\mathcal{F}_s}[M_t] \geq M_s, \quad \mathbb{P}-\text{a.s.\! for all } s,t \in I \text{ with } s\leq t.
        \end{align}
        \item<3-> Analogously for supermartingale.
    \end{itemize}
\end{frame2}

\subsection{One-dimensional Brownian Motions}

\begin{frame2}
    If $B$ is a one-dimensional $(\mathcal{F}_t)_{t\geq 0}$-Brownian motion, then $B$ is a $(\mathcal{F}_t)_{t\geq 0}$-martingale.

    In particular, every one-dimensional standard Brownian motion $B$ is a martingale with respect to its natural filtration $(\mathcal{B}_t^B)_{t\geq 0}$.
    \vspace{5pt}
    \hrule
    \vspace{5pt}
    %\task{Repeat Remark 6.7}
    Remark 6.7. Let $(M_t)_{t \in I}$ be a $(\mathcal{F}_t)_{t\in I}$-martingale and denote by $(\Omega, \tilde{\mathcal{F}},(\tilde{\mathcal{F}}_t)_{t\in I},\tilde{\mathbb{P}})$ the completion of $(\Omega, \mathcal{F},(\mathcal{F}_t)_{t\in I},\mathbb{P})$.
    Then $(M_t)_{t\in I}$ is a $(\tilde{\mathcal{F}}_t)_{t\in I}$-martingale as well, provided that $(\Omega, \tilde{\mathcal{F}},(\tilde{\mathcal{F}}_t)_{t\in I},\tilde{\mathbb{P}})$ is chosen as the underlying filtered probability space.
    \vspace{5pt}
    \hrule
    \vspace{5pt}
    By virtue of Remark 6.7 we can further conclude that every one-dimensional standard Brownian motion $B$ is a martingale on the standard filtered probability space $(\Omega, \tilde{\mathcal{F}},(\tilde{\mathcal{F}_t^B})_{t\geq 0},\tilde{\mathbb{P}})$ obtained by completing $(\Omega, \mathcal{F},(\mathcal{F}_t^B)_{t\geq 0},\mathbb{P})$.
\end{frame2}

\begin{frame2}
    \begin{itemize} 
        \item Let us first recall that
        \begin{align}
            \int_{\Omega} |B_t| \, d\mathbb{P} &= \frac{1}{(2\pi t)^{1/2}}\int_\mathbb{R} |x| e^{-x^2/2t} \, dx < \infty, \\
            \mathbb{E}[B_t] &= \frac{1}{(2\pi t)^{1/2}}\int_\mathbb{R}xe^{-x^2/2t} \, dx,
        \end{align}
        for every $t > 0$.
    \end{itemize}
\end{frame2}

\begin{frame2}
    \begin{itemize}
        \item Since $B_0 = 0$, $\mathbb{P}-$a.s., it follows in particular that $B_t$ is $\mathbb{P}$-integrable with $\mathbb{E}[B_t] = 0$ for all $t \geq 0$.
        \item For all $0 \leq s \leq t < \infty$, we further observe that, $\mathbb{P}-$a.s.,
        \begin{align}
            \mathbb{E}^{\mathcal{F}_s}[B_t] &= \mathbb{E}^{\mathcal{F}_s}[B_t - B_s] + \mathbb{E}^{\mathcal{F}_s} \\
             &= \mathbb{E}[B_t - B_s] + B_s = \mathbb{E}[B_t] - \mathbb{E}[B_s] + B_s, 
        \end{align}
        since the increment $B_t - B_s$ is $\mathcal{F}_s$-independent and $B_s$ is $\mathcal{F}_s$-measurable.
    \end{itemize}
\end{frame2}

\section{Stochastic Integrals}

\subsection{Strategy}
\begin{frame2}
    \begin{itemize}
        \item 
    \end{itemize}
\end{frame2}

\section{Itô's Formula}

\subsection{Strategy}
\begin{frame2}
    \begin{itemize}
        \item Itô's formula for semi martingales (+ any necessary preceedings)
        \item Exercise 8 (Part of Example 4)
    \end{itemize}
\end{frame2}

\begin{frame2}
    \textbf{Theorem 7.}
    %Let $X,Y \in \mathcal{SM}^c$ with decomposition $(M,A),(\bar{M},\bar{A}) \in \mathcal{M}_{c,loc} \times A_{loc}$, respectively.
    %The mapping $H \in \mathcal{L}(X) \mapsto H \cdot X \in \mathcal{SM}^c$ enjoys the following properties (up to indishtinguishability)
    %\begin{enumerate}
        %\item 
        Let $X = (X^{(1)},\ldots,X^{(d)})$, where $X^{(i)} \in \mathcal{SM}^c$, for all $i = 1,\ldots,d$.
        If $f \, : \, \mathbb{R}^d \rightarrow \mathbb{R}$ is twice continuously differentiable and $X = (X^{(1)},\ldots,X^{(d)})$, then
        \begin{align*}
            f(X_t) = f(X_0) &+ \sum_{i = 1}^d\int_0^t\partial_{x_i}f(X_s)\, dX_s \\
            &\quad+ \frac{1}{2}\sum_{i,j=1}^d\int_0^t\partial_{x_i}\partial_{x_j}f(X_s)\, d \langle X^{(i)},X^{(j)}\rangle_s.
        \end{align*}
    %\end{enumerate} 
\end{frame2}

\begin{frame2}
    \textbf{Example 4 (Complex Exponential Process).}
    Let $M \in \mathcal{M}_{c,loc}$ starting at 0 and define for $t \geq 0$
    \begin{align}
        N_t^{(1)} := \cos(M_t)e^{\frac{1}{2}\langle M \rangle_t},\ N_t^{(2)} := \sin(M_t)e^{\frac{1}{2}\langle M \rangle_t}.
    \end{align}
    Verify that
    \begin{align}
        N_t^{(2)} = \int_0^t N_s^{(1)} \, dM_s, \ t \geq 0.
    \end{align}
\end{frame2}

\begin{frame2}
    We will start by observing that Itô's formula applied to the function $f(x,y) = xy$ results in the following integration by parts formula
    \begin{align*}
        X_tY_t = X_0Y_0 + \int_0^t X_s \, dY_s + \int_0^tY_s\, dX_s + \langle X,Y \rangle_t, \ \forall\, X,Y \in \mathcal{SM}^c.
    \end{align*}
    Thus we get that
    \begin{align}
         N_t^{(2)} &= \sin(M_t)e^{\frac{1}{2}\langle M \rangle_t} \\
        &= \int_0^t \sin(M_s) \, d\left(e^{\frac{1}{2}\langle M \rangle_s}\right) + \int_0^t e^{\frac{1}{2}\langle M \rangle_s} \, d \left(\sin(M_s)\right).
    \end{align}
\end{frame2}

\begin{frame2}
    Applying Itô's formula again we get
    \begin{align}
        e^{\frac{1}{2}\langle M \rangle_t} &= 1 + \int_0^t\partial_{\langle M \rangle_s}e^{\frac{1}{2}\langle M \rangle_s}\, d\langle M \rangle_s + \frac{1}{2}\int_0^t\partial^2_{\langle M \rangle_s} e^{\frac{1}{2}\langle M \rangle_s} \, d \langle\langle M \rangle\rangle_s \nonumber \\
        &= 1 + \frac{1}{2}\int_0^te^{\frac{1}{2}\langle M \rangle_s}\, d\langle M \rangle_s
    \end{align}
    \begin{align}
        \sin(M_t) &= \int_0^t\partial_{M_s}\sin(M_s)\, dM_s + \frac{1}{2}\int_0^t\partial_{M_s}^2\sin(M_s) \, d \langle M \rangle_s \\
        &= \int_0^t \cos(M_s) \, dM_s - \frac{1}{2}\int_0^t\sin(M_s)\, d\langle M \rangle_s.
    \end{align}
\end{frame2}

\begin{frame2}
    Now inserting what we just found
    \begin{align*}
        N_t^{(2)} &= \sin(M_t)e^{\frac{1}{2}\langle M \rangle_t} \\
       &= \int_0^t \sin(M_s) \, d\left(e^{\frac{1}{2}\langle M \rangle_s}\right) + \int_0^t e^{\frac{1}{2}\langle M \rangle_s} \, d \left(\sin(M_s)\right) \\
       &= \frac{1}{2}\int_0^t\sin(M_s)e^{\frac{1}{2}\langle M \rangle_s}\, d\langle M \rangle_s + \int_0^t\cos(M_s)e^{\frac{1}{2}\langle M \rangle_s}\, dM_s \\
       &\qquad - \frac{1}{2}\int_0^t\sin(M_s)e^{\frac{1}{2}\langle M \rangle_s}\, d\langle M \rangle_s \\
       &= \int_0^t\cos(M_s)e^{\frac{1}{2}\langle M \rangle_s}\, dM_s \\
       &= \int_0^tN_t^{(1)} \, dM_s.
   \end{align*}
\end{frame2}

\section{Girsanov-Transformation}

\subsection{Strategy}
\begin{frame2}
    \begin{itemize}
        \item Theorem 9
        \item Proposition 3
        \item Theorem 10
        \item Exercise 10
    \end{itemize}
\end{frame2}

\subsection{Theorem 9}

\begin{frame2}
    \textbf{Theorem 9.}
    Let $0 < T \leq +\infty$ and $M = (M_t)_{0\leq t \leq T}$ be a complex-valued continuous local martingale.
    The following then holds
    \begin{enumerate}
        \item If $\mathbb{E}(\sup_{0 \leq s \leq t} \| M_s \|) < \infty$ for every $ 0 \leq t \leq T$, then $M$ is a true martingale.
        \item Suppose that $M$ is real-valued.
        For all $0 \leq t \leq T$, $\mathbb{E}(\langle M \rangle_t) < \infty$ if and only if $M$ is a square integrable martingale.
        In this case $M^2 - \langle M \rangle$ is a true martingale.
    \end{enumerate}
\end{frame2}

\subsection{Proposition 3}

\begin{frame2}
    \textbf{Proposition 3.}
    Let $M \in \mathcal{M}_{c,loc}$ and consider $0 < T \leq + \infty$.
    The complex-valued process defined as
    \begin{align}
        N_t := e^{\textbf{i}M_t + \frac{1}{2}\langle M \rangle_t}, \ t \geq 0,
    \end{align}
    is a continuous local martingale.
    If $\mathbb{E}\left(e^{\frac{1}{2}\langle M \rangle_t}\right) < \infty$ for all $t \leq T$, then $N$ is a true martingale.
\end{frame2}

\subsection{Theorem 10}

\begin{frame2}
    \textbf{Theorem 10 (Lévy).}
    Fix $d \in \mathbb{N}$.
    If $M^1,\ldots,M^d$ are elements of $\mathcal{M}_{c,loc}$ starting at 0 almost surely satisfying that
    \begin{align}
        \langle M^i,M^j \rangle_t = \delta^{ij}t, \ t \geq 0,
    \end{align}
    then $M = (M^1, \ldots, M^d)$ is an $(\mathcal{F}_t)_{t \geq 0}$-Brownian motion.
\end{frame2}

\subsection{Exercise 10}

\begin{frame2}
    \textbf{Exercise 10.}
    Apply Theorem 9 to show that Proposition 3 holds.
    Use this to show that the complex-valued process defined as
    \begin{align}
        \xi_t := exp\{ \textbf{i}N_t + \frac{1}{2}\| \lambda \|^2 t \}, \ t \geq 0,
    \end{align}
    is a true martingale, where
    \begin{align}
        N := \sum_{j = 1}^d \lambda_j M^j, \ \lambda_1,\ldots,\lambda_d \in \mathbb{R}^d,
    \end{align}
    in which $M_1, M_2, \ldots, M^d$ are as in Theorem 10.
\end{frame2}

\begin{frame2}
    \textbf{Solution.}
    For the first part we need to show that
    \begin{align}
        \mathbb{E}\left[e^{\frac{1}{2}\langle M \rangle_t}\right] < \infty \Rightarrow \mathbb{E}\left[\sup_{0 \leq s \leq t}\| N_s \|\right] < \infty,
    \end{align}
    where
    \begin{align}
        N_t := e^{\textbf{i}M_t + \frac{1}{2}\langle M \rangle_t}.
    \end{align}
\end{frame2}

\begin{frame2}
    Let us start by looking at
    \begin{align}
        \mathbb{E}\left[\sup_{0 \leq s \leq t} \| N_s \| \right] &= \mathbb{E}\left[\sup_s \| e^{\textbf{i}M_s + \frac{1}{2}\langle M \rangle_s} \|\right]\\
        &= \mathbb{E}\left[\sup_s \| e^{\textbf{i}M_s} \| \cdot \|e^{\frac{1}{2}\langle M \rangle_s} \|\right] \\
        &= \mathbb{E}\left[\sup_s \|e^{\frac{1}{2}\langle M \rangle_s}\|\right] = \sup_s \mathbb{E}\left[ \|e^{\frac{1}{2}\langle M \rangle_s} \|\right] < \infty
    \end{align}
    This concludes that $N_t$ is a true martingale. 
\end{frame2}

\begin{frame2}
    For the second part we first have that $N$ is a linear combination of $\mathcal{M}_{c,loc} \Rightarrow N \in \mathcal{M}_{c,loc}$.
    
    \vspace{10pt}
    Further we have (from the proof of theorem 10) that
    \begin{align}
        \langle N \rangle_t = \| \lambda \|^2 \cdot t,
    \end{align}
    thereby $\xi_t$ is of the form in proposition 3.
\end{frame2}

\end{document}
